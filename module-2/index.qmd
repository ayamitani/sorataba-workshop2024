---
title: "Module 2: Marginal models"
author: "Aya Mitani"
format:
  revealjs:
    theme: theme.scss
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    embed-resources: true
---

```{r}
library(here)
library(tidyverse)
library(geepack)
library(lme4)
library(kableExtra)
knitr::opts_knit$set(root.dir = here())
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
theme_set(theme_bw(base_size = 20)) # Use black/white theme and increase font size for all ggplot 
```

# Review of Generalized linear models (GLM)

## Generalized linear models (GLM){.smaller}

* **Generalized linear models (GLM)** are a class of parametric statistical models for the conditional distribution of a response $Y_i$ given a $p$-vector of covariates $X_i$:
  1) probability distribution, $Y_i \sim f_Y(y)$
  2) linear predictor, $\eta_i = X_i^{T}\beta$
  3) link function, $g(\cdot)$ such that $g(\mu_i) = \eta_i$ where $\mu_i = E(Y_i|X_i)$
* The theory of GLMs focuses on distributions that belong to the *exponential dispersion family*
  - Bernoulli and Poisson distributions are both in the exponential family

$$
f_Y(y_i;\theta_i,\phi) = \exp\left\{\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)}+c(y_i,\phi)\right\}
$$

## Examples of GLM{.smaller}

  - Logistic regression for binary responses
  - Log-linear regression for counts
  - Proportional odds model for ordinal responses

<br>

| Response type | Canonical link | Mean function                          | Variance function |
|---------------|----------------|----------------------------------------|-------------------|
| Continuous    | Identity       | $\mu_{ij}=X^{T}_{ij}\beta$             | $\phi$            |
| Binary        | Logit          | $\text{log}\left(\frac{\mu_{ij}}{1-\mu_{ij}}\right)=X^{T}_{ij}\beta$ | $\mu_{ij}(1-\mu_{ij})$ ($\phi=1$)|
| Count         | Log            | $\text{log}(\mu_{ij})=X^{T}_{ij}\beta$ | $\phi \mu_{ij}$                    |                   


## GLM in R
```{r}
#| echo: TRUE
#| eval: FALSE

## see 
?family

## logistic regression
glm(y ~ x1 + x2, family = binomial(link = "logit"), data = mydata)

## Poisson regression
glm(y ~ x1 + x2, family = poisson(link = "log"), data = mydata)
```


## Marginal models for correlated data {.smaller}

* One extension of GLM to correlated data
* Marginal models for a **continuous** response is a special case of **general linear model** 
<!-- * For discrete **repeated** responses, the interpretation of regression coefficients from marginal models is not the same from mixed effects models -->
<!--   - Marginal model $=$ Population-average model -->
<!--   - Mixed effects model $=$ Subject-specific model -->
* Marginal models are primarily used to make inferences about **population means**
* The goal is to make inference about the mean response
  - The model for the mean response depends only on the covariates of interest ($X_{ij}$), and not on any random effects or previous responses
* The within-subject association among the repeated measures are considered as a "nuisance" characteristic
  + But must be accounted for to make correct inferences about changes in the population mean response
  
  
## Three-part specifications{.smaller}

1. The marginal expectation of the response, $E(Y_{ij}|X_{ij}) = \mu_{ij}$, depends on the covariates, $X_{ij}$, through a known link function
$$
g(\mu_{ij}) = \eta_{ij} = X^{T}_{ij}\beta.
$$

2. The variance of each $Y_{ij}$, given the covariates, depends on the mean according to 
$$
\text{Var}(Y_{ij}|X_{ij}) = \phi v(\mu_{ij}),
$$
where $v(\mu_{ij})$ is a known "variance function" and $\phi$ is a scale parameter that may be known or need to be estimated.


## Three-part specifications (continued) {.smaller}

3. The **pairwise** within-subject association among the vector of repeated responses, given the covariates, is assumed to be a function of the means, $\mu_{ij}$, and an additional set of within-subject association parameters, $\alpha$. The covariance matrix of the repeated responses can be constructed as the product of standard deviations and correlations
$$
V_i = A^{1/2}_{i}\text{Corr}(Y_{i})A^{1/2}_{i},
$$
where $A_{i}$ is a diagonal matrix with $\text{Var}(Y_{ij}|X_{ij})=\phi v(\mu_{ij})$ along the diagonal and $\text{Corr}(Y_{i})$ is the correlation matrix which is a function of $\alpha$. 


## Generalized estimating equations {.smaller}

* In marginal models, we don't make any assumptions about the full distributional property of the response $Y_{ij}$
  + We only specify the first two moments of the distribution of $Y_{ij}$
  + Therefore, the analysis is **semi-parametric**
* We cannot specify a likelihood $\rightarrow$ cannot use MLE for estimation/inference
* We construct an unbiased estimating equation and derive the asymptotic sampling distribution of the resulting estimator

The objective is to minimize the function:
$$
\sum_{i=1}^{N}\left\{y_{i}-\mu_{i}(\beta)\right\}^{T}V_{i}(\beta, \alpha, \phi)^{-1}\left\{y_{i}-\mu_{i}(\beta)\right\}
$$
with respect to $\beta$, where $V_{i}$ is treated as known.

## Generalized estimating equations {.smaller}

If a minimum of the above function exists, then it must solve the following **generalized estimating equations**:
$$
\sum_{i=1}^{N}D^{T}_{i}(\beta)V_{i}(\beta, \alpha, \phi)^{-1}\left\{y_{i}-\mu_{i}(\beta)\right\}=0
$$
where

* $V_{i}$ is the so-called $n_{i} \times n_{i}$ "working" covariance matrix
* $D_{i} = \partial \mu_{i}/\partial \beta$ is the $n_{i} \times p$ matrix containing the derivatives of $\mu_{i}$ with respect to the components of $\beta$

## Generalized estimating equations {.smaller}

In simple form, the GEE can be expressed as
$$
\sum_{i=1}^{N}D^{T}_{i}V_{i}^{-1}(y_{i}-\mu_{i})=0.
$$

* The GEE are functions of both $\beta$ and $\alpha$
* The GEE have no closed-form solution (except for the identity link function)
* Finding the solution for $\beta$ requires an **iterative algorithm** (e.g. Fisher's scoring algorithm)

## Estimation of $\phi$ and $\alpha$ {.smaller}

* Method to estimate ($\phi, \alpha$) is still a developing field
* Two methods that are widely adoped are:
  + Moment based estimation -- used in R package `gee`
  + Estimating equations for $\alpha$ -- used in R package `geeglm`

## Properties of $\hat \beta$ {.smaller}

* $\hat \beta$ is a consistent estimator of $\beta$ (bias of $\hat \beta$ decreases to zero as the sample size increases) even if the working covariance model does not correspond to the true covariance model
  + $\hat \beta$ is **robust** to misspecification of the variance-covariance structure (in large samples, at least)
* Selecting a working covariance model that is *close* to the true covariance model will yield a more **efficient** estimator

The expression for $\widehat{\text{Cov}}(\hat\beta)$ is given by

$$
\small\left(\sum_{i=1}^{N}\hat D_{i}^{T}\hat V_{i}^{-1}\hat D_{i}\right)^{-1}
\left\{\sum_{i=1}^{N}\hat D_{i}^{T}\hat V_{i}^{-1}(Y_{i}-\hat\mu_{i})(Y_{i}-\hat\mu_{i})^{T}\hat V_{i}^{-1}\hat D_{i}\right\}
\left(\sum_{i=1}^{N}\hat D_{i}^{T}\hat V_{i}^{-1}\hat D_{i}\right)^{-1}
$$
This is known as the **empirical** or **sandwich** estimator.


## Sandwich estimator of $\text{Cov}(\hat\beta)$ {.smaller}

* The sandwich estimator provides **valid standard errors** even when the assumed model for the covariances among the repeated measures is incorrect
* With **large sample sizes**, the sandwich estimator of $\text{Cov}(Y_{i})$ yields correct standard errors
* For this reason, the SEs from the sandwich estimator is often referred to as the **robust** standard errors (or **empirical** standard errors)

So why bother modeling the within-subject association (specifying the working covariance matrix)???

1. The closer the working covariance matrix approximates the true underlying covariance matrix, the greater the **efficiency** or **precision** with which $\beta$ can be estimated
2. The robustness property of the sandwich estimator is a **large sample** property

## Example: Respiratory study {.smaller}

The data are from a clinical trial of patients with respiratory illness, where 111 patients from two different clinics were randomized to receive either placebo or an active treatment. Patients were examined at baseline and at four visits during treatment. At each
examination, respiratory status (categorized as 1 = good, 0 = poor) was determined. 

|Variable   |Value    |Description                                                    |
|:----------|:--------|:--------------------------------------------------------------|
|clinic     |Numeric  |Clinic ID                                                      |
|id         |Numeric  |Subject ID                                                     |
|trt        |Numeric  |Treatment (A=Active, P=Placebo)                                |
|y0         |Numeric  |Baseline Response                                              |
|y1         |Numeric  |Visit 1 Response                                               |
|y2         |Numeric  |Visit 2 Response                                               |
|y3         |Numeric  |Visit 3 Response                                               |
|y4         |Numeric  |Visit 4 Response                                               |

Table: Data dictionary of data from respiratory study

## Example: Respiratory study
```{r}
#| echo: TRUE

resp <- read.table(here("datasets", "respir.txt"), header = TRUE)
head(resp)
length(unique(resp$id))
```
This data set is in a **wide** format.

## Example: Respiratory study{.smaller}
```{r}
#| echo: TRUE
respl <- resp %>%
  pivot_longer(cols = starts_with("y"),
               names_to = "visit",
               names_prefix = "y",
               values_to = "resp") %>%
  mutate(visit = as.numeric(visit),
         trt1 = ifelse(trt == "A", 1, 0),
         clinic2 = ifelse(clinic == 2, 1, 0))
head(respl, 20)
```


## Example: Descriptive analysis
Baseline data by clinic and treatment
```{r}
#| echo: TRUE
respl %>%
  filter(visit == 0) %>%
  group_by(clinic, trt) %>%
  summarise(avgresp = mean(resp))
```


## Example: Descriptive analysis{.smaller}
Percent of good respiratory status over time by clinic
```{r}
#| echo: TRUE
respl %>%
  group_by(visit, trt, clinic) %>%
  mutate(pcty = mean(resp)) %>%
  ggplot(aes(y = pcty, x = visit, color = as.factor(trt))) +
  geom_point(size = 2.5) + 
  geom_line(size = 1.5) + 
  facet_grid(~clinic) + 
  ylim(0,1) + 
  labs(y = "Percent of good respiratory status", x = "Visit", color = "Treatment")
```
