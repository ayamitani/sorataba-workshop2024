---
title: "Module 2: Marginal models"
author: "Aya Mitani"
format:
  revealjs:
    theme: theme.scss
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    embed-resources: true
---

```{r}
library(here)
library(tidyverse)
library(nlme)
library(lme4)
library(geepack)
library(kableExtra)
library(gtsummary)
library(texreg)
library(AICcmodavg)
knitr::opts_knit$set(root.dir = here())
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
theme_set(theme_bw(base_size = 20)) # Use black/white theme and increase font size for all ggplot 
```

# Review of Generalized linear models (GLM)

## Generalized linear models (GLM){.smaller}

* **Generalized linear models (GLM)** are a class of parametric statistical models for the conditional distribution of a response $Y_i$ given a $p$-vector of covariates $X_i$:
  1) probability distribution, $Y_i \sim f_Y(y)$
  2) linear predictor, $\eta_i = X_i^{T}\beta$
  3) link function, $g(\cdot)$ such that $g(\mu_i) = \eta_i$ where $\mu_i = E(Y_i|X_i)$
* The theory of GLMs focuses on distributions that belong to the *exponential dispersion family*
  - Bernoulli and Poisson distributions are both in the exponential family

$$
f_Y(y_i;\theta_i,\phi) = \exp\left\{\frac{y_i\theta_i - b(\theta_i)}{a_i(\phi)}+c(y_i,\phi)\right\}
$$

## Examples of GLM{.smaller}

  - Logistic regression for binary responses
  - Log-linear regression for counts
  - Proportional odds model for ordinal responses

<br>

| Response type | Canonical link | Mean function                          | Variance function |
|---------------|----------------|----------------------------------------|-------------------|
| Continuous    | Identity       | $\mu_{ij}=X^{T}_{ij}\beta$             | $\phi$            |
| Binary        | Logit          | $\text{log}\left(\frac{\mu_{ij}}{1-\mu_{ij}}\right)=X^{T}_{ij}\beta$ | $\mu_{ij}(1-\mu_{ij})$ ($\phi=1$)|
| Count         | Log            | $\text{log}(\mu_{ij})=X^{T}_{ij}\beta$ | $\phi \mu_{ij}$                    |                   


## GLM in R
```{r}
#| echo: TRUE
#| eval: FALSE

## see 
?family

## logistic regression
glm(y ~ x1 + x2, family = binomial(link = "logit"), data = mydata)

## Poisson regression
glm(y ~ x1 + x2, family = poisson(link = "log"), data = mydata)
```


## Marginal models for correlated data {.smaller}

* One extension of GLM to correlated data
* Marginal models for a **continuous** response is a special case of **general linear model** 
<!-- * For discrete **repeated** responses, the interpretation of regression coefficients from marginal models is not the same from mixed effects models -->
<!--   - Marginal model $=$ Population-average model -->
<!--   - Mixed effects model $=$ Subject-specific model -->
* Marginal models are primarily used to make inferences about **population means**
* The goal is to make inference about the mean response
  - The model for the mean response depends only on the covariates of interest ($X_{ij}$), and not on any random effects or previous responses
* The within-subject association among the repeated measures are considered as a "nuisance" characteristic
  + But must be accounted for to make correct inferences about changes in the population mean response
  
  
## Three-part specifications{.smaller}

1. The marginal expectation of the response, $E(Y_{ij}|X_{ij}) = \mu_{ij}$, depends on the covariates, $X_{ij}$, through a known link function
$$
g(\mu_{ij}) = \eta_{ij} = X^{T}_{ij}\beta.
$$

2. The variance of each $Y_{ij}$, given the covariates, depends on the mean according to 
$$
\text{Var}(Y_{ij}|X_{ij}) = \phi v(\mu_{ij}),
$$
where $v(\mu_{ij})$ is a known "variance function" and $\phi$ is a scale parameter that may be known or need to be estimated.


## Three-part specifications (continued) {.smaller}

3. The **pairwise** within-subject association among the vector of repeated responses, given the covariates, is assumed to be a function of the means, $\mu_{ij}$, and an additional set of within-subject association parameters, $\alpha$. The covariance matrix of the repeated responses can be constructed as the product of standard deviations and correlations
$$
V_i = A^{1/2}_{i}\text{Corr}(Y_{i})A^{1/2}_{i},
$$
where $A_{i}$ is a diagonal matrix with $\text{Var}(Y_{ij}|X_{ij})=\phi v(\mu_{ij})$ along the diagonal and $\text{Corr}(Y_{i})$ is the correlation matrix which is a function of $\alpha$. 


## Generalized estimating equations {.smaller}

* In marginal models, we don't make any assumptions about the full distributional property of the response $Y_{ij}$
  + We only specify the first two moments of the distribution of $Y_{ij}$
  + Therefore, the analysis is **semi-parametric**
* We cannot specify a likelihood $\rightarrow$ cannot use MLE for estimation/inference
* We construct an unbiased estimating equation and derive the asymptotic sampling distribution of the resulting estimator

The objective is to minimize the function:
$$
\sum_{i=1}^{N}\left\{y_{i}-\mu_{i}(\beta)\right\}^{T}V_{i}(\beta, \alpha, \phi)^{-1}\left\{y_{i}-\mu_{i}(\beta)\right\}
$$
with respect to $\beta$, where $V_{i}$ is treated as known.

## Generalized estimating equations {.smaller}

If a minimum of the above function exists, then it must solve the following **generalized estimating equations**:
$$
\sum_{i=1}^{N}D^{T}_{i}(\beta)V_{i}(\beta, \alpha, \phi)^{-1}\left\{y_{i}-\mu_{i}(\beta)\right\}=0
$$
where

* $V_{i}$ is the so-called $n_{i} \times n_{i}$ "working" covariance matrix
* $D_{i} = \partial \mu_{i}/\partial \beta$ is the $n_{i} \times p$ matrix containing the derivatives of $\mu_{i}$ with respect to the components of $\beta$

## Generalized estimating equations {.smaller}

In simple form, the GEE can be expressed as
$$
\sum_{i=1}^{N}D^{T}_{i}V_{i}^{-1}(y_{i}-\mu_{i})=0.
$$

* The GEE are functions of both $\beta$ and $\alpha$
* The GEE have no closed-form solution (except for the identity link function)
* Finding the solution for $\beta$ requires an **iterative algorithm** (e.g. Fisher's scoring algorithm)

## Estimation of $\phi$ and $\alpha$ {.smaller}

* Method to estimate ($\phi, \alpha$) is still a developing field
* Two methods that are widely adoped are:
  + Moment based estimation -- used in R package `gee`
  + Estimating equations for $\alpha$ -- used in R package `geeglm`

## Properties of $\hat \beta$ {.smaller}

* $\hat \beta$ is a consistent estimator of $\beta$ (bias of $\hat \beta$ decreases to zero as the sample size increases) even if the working covariance model does not correspond to the true covariance model
  + $\hat \beta$ is **robust** to misspecification of the variance-covariance structure (in large samples, at least)
* Selecting a working covariance model that is *close* to the true covariance model will yield a more **efficient** estimator

The expression for $\widehat{\text{Cov}}(\hat\beta)$ is given by

$$
\small\left(\sum_{i=1}^{N}\hat D_{i}^{T}\hat V_{i}^{-1}\hat D_{i}\right)^{-1}
\left\{\sum_{i=1}^{N}\hat D_{i}^{T}\hat V_{i}^{-1}(Y_{i}-\hat\mu_{i})(Y_{i}-\hat\mu_{i})^{T}\hat V_{i}^{-1}\hat D_{i}\right\}
\left(\sum_{i=1}^{N}\hat D_{i}^{T}\hat V_{i}^{-1}\hat D_{i}\right)^{-1}
$$
This is known as the **empirical** or **sandwich** estimator.


## Sandwich estimator of $\text{Cov}(\hat\beta)$ {.smaller}

* The sandwich estimator provides **valid standard errors** even when the assumed model for the covariances among the repeated measures is incorrect
* With **large sample sizes**, the sandwich estimator of $\text{Cov}(Y_{i})$ yields correct standard errors
* For this reason, the SEs from the sandwich estimator is often referred to as the **robust** standard errors (or **empirical** standard errors)

So why bother modeling the within-subject association (specifying the working covariance matrix)???

1. The closer the working covariance matrix approximates the true underlying covariance matrix, the greater the **efficiency** or **precision** with which $\beta$ can be estimated
2. The robustness property of the sandwich estimator is a **large sample** property

## Example: Respiratory study {.smaller}

The data are from a clinical trial of patients with respiratory illness, where 111 patients from two different clinics were randomized to receive either placebo or an active treatment. Patients were examined at baseline and at four visits during treatment. At each
examination, respiratory status (categorized as 1 = good, 0 = poor) was determined. 

::: {style="font-size: 75%;"}

|Variable   |Value    |Description                                                    |
|:----------|:--------|:--------------------------------------------------------------|
|clinic     |Numeric  |Clinic ID                                                      |
|id         |Numeric  |Subject ID                                                     |
|trt        |Numeric  |Treatment (A=Active, P=Placebo)                                |
|y0         |Numeric  |Baseline Response                                              |
|y1         |Numeric  |Visit 1 Response                                               |
|y2         |Numeric  |Visit 2 Response                                               |
|y3         |Numeric  |Visit 3 Response                                               |
|y4         |Numeric  |Visit 4 Response                                               |

Table: Data dictionary of data from respiratory study

:::

## Example: Respiratory study
```{r}
#| echo: TRUE

resp <- read.table(here("datasets", "respir.txt"), header = TRUE)
head(resp)
length(unique(resp$id))
```
This data set is in a **wide** format.

## Example: Respiratory study{.smaller}
```{r}
#| echo: TRUE
respl <- resp %>%
  pivot_longer(cols = starts_with("y"),
               names_to = "visit",
               names_prefix = "y",
               values_to = "resp") %>%
  mutate(visit = as.numeric(visit),
         trt1 = ifelse(trt == "A", 1, 0),
         clinic2 = ifelse(clinic == 2, 1, 0))
head(respl, 20)
```


## Example: Descriptive analysis
Baseline data by clinic and treatment
```{r}
#| echo: TRUE
respl %>%
  filter(visit == 0) %>%
  group_by(clinic, trt) %>%
  summarise(avgresp = mean(resp))
```


## Example: Descriptive analysis{.smaller}
Percent of good respiratory status over time by clinic
```{r}
#| echo: TRUE
respl %>%
  group_by(visit, trt, clinic) %>%
  mutate(pcty = mean(resp)) %>%
  ggplot(aes(y = pcty, x = visit, color = as.factor(trt))) +
  geom_point(size = 2.5) + 
  geom_line(size = 1.5) + 
  facet_grid(~clinic) + 
  ylim(0,1) + 
  labs(y = "Percent of good respiratory status", x = "Visit", color = "Treatment")
```

## Example: Observations and questions{.smaller}

* Patients in clinic 2 have higher percentage of good respiratory status
* Within each clinic, percentages of good respiratory status at baseline are similar between treatment groups
* In both clinics, we observe an increase in percentage of good respiratory status in the treatment arms
* Does the rate of change in good respiratory status differ between the two treatments? Between clinics?

$$
\begin{aligned}
\text{logit}\left\{\Pr(Y_{ij}=1)\right\} &= \beta_{1} + \beta_{2}\text{Clinic}_{i} + \beta_{3}\text{Visit}_{ij}+\beta_{4}\text{Trt}_{i} + \\ & \beta_{5}\text{Clinic}_{i}\times \text{Visit}_{ij} + \beta_{6}\text{Clinic}_{i}\times \text{Trt}_{i} + \beta_{7}\text{Visit}_{ij} \times \text{Trt}_{i} + \\ 
& \beta_{8}\text{Clinic}_{i} \times \text{Visit}_{ij} \times \text{Trt}_{i}
\end{aligned}
$$
where $\text{Clinic}_{i} = 1$ for clinic 2 and $0$ otherwise, and $\text{Trt}_{i} = 1$ for the treatment arm and $0$ otherwise.

## Example: Model interpretation{.smaller}

With this model, the mean response for patients in clinic 1 and placebo arm is:
$$
\text{logit}\left\{\Pr(Y_{ij}=1)\right\} = \beta_{1} + \beta_{3}\text{Visit}_{ij}.
$$
The mean response for patients in clinic 1 and treatment arm is:
$$
\text{logit}\left\{\Pr(Y_{ij}=1)\right\} = (\beta_{1} + \beta_{4}) + (\beta_{3} + \beta_{7})\text{Visit}_{ij}.
$$
The mean response for patients in clinic 2 and placebo arm is:
$$
\text{logit}\left\{\Pr(Y_{ij}=1)\right\} = (\beta_{1} + \beta_{2}) + (\beta_{3} + \beta_{5})\text{Visit}_{ij} 
$$
Finally, the mean response for patient in clinic 2 and treatment arm is:
$$
\text{logit}\left\{\Pr(Y_{ij}=1)\right\} = (\beta_{1} + \beta_{2} + \beta_{4} + \beta_{6}) + (\beta_{3} + \beta_{5} + \beta_{7}+ \beta_{8})\text{Visit}_{ij} 
$$

## Model building{.smaller}

Diggle, Heagerty, Liang and Zeger (2002)

> When regression coefficients are the scientific focus as in the examples
here, one should invest the lionâ€™s share of time in modeling the mean
structure, while using a reasonable approximation to the covariance. The
robustness of the inferences about can be checked by fitting a final model
using different covariance assumptions and comparing the two sets of
estimates and their robust standard errors. If they differ substantially, a
more careful treatment of the covariance model may be necessary.

#### Strategy

1. Model the mean structure assuming unstructured working correlation structure
2. Compare models between different working correlation structures
  

## Model mean structure

* Because we are working with **binary** data we need to include the argument: `scale.fix=TRUE`
* This option keeps the scale parameter **fixed** to its default value of 1 rather than estimated. 

Code for three-way interaction model with unstructured working covariance
```{r}
#| echo: TRUE
#| eval: FALSE
model1 <- geeglm(resp ~ trt1*visit*clinic2,
                 id = id,
                 family = binomial(link = "logit"),
                 corstr = "unstructured",
                 scale.fix = TRUE,
                 data = respl)
summary(model1)
```


## Model mean structure

Output from three-way interaction model with unstructured working covariance

```{r}
#| echo: FALSE
#| eval: TRUE
model1 <- geeglm(resp ~ trt1*visit*clinic2,
                 id = id,
                 family = binomial(link = "logit"),
                 corstr = "unstructured",
                 scale.fix = TRUE,
                 data = respl)
summary(model1)
```

Do not reject $H_0: \beta_{8} = 0$

## Model mean structure

Code for three two-way interactions model with unstructured working covariance
```{r}
#| echo: TRUE
#| eval: FALSE
model2 <- geeglm(resp ~ trt1*visit + clinic2*visit + trt1*clinic2,
                 id = id,
                 family = binomial(link = "logit"),
                 corstr = "unstructured",
                 scale.fix = TRUE,
                 data = respl)
summary(model2)
```
The two-way interactions are not important.

## Model covariance {.smaller}
::: {.column width="49%"}

No interaction model with various working covariances

* Use QIC (quasi-likelihood information criterion) to select a correlation structure
  + Analogous to AIC for quasi-likelihood
  + QIC can be used to compare models with different mean structures and different working correlation structures
  + QICu is only used to compare models with different mean structures but the same working correlation structure

:::

::: {.column width="49%"}
```{r}
#| echo: TRUE
model3un <- geeglm(resp ~ trt1 + visit + clinic2,
                 id = id,
                 family = binomial(link = "logit"),
                 corstr = "unstructured",
                 scale.fix = TRUE,
                 data = respl)

model3ex <- geeglm(resp ~ trt1 + visit + clinic2,
                 id = id,
                 family = binomial(link = "logit"),
                 corstr = "exchangeable",
                 scale.fix = TRUE,
                 data = respl)

model3ar1 <- geeglm(resp ~ trt1 + visit + clinic2,
                   id = id,
                   family = binomial(link = "logit"),
                   corstr = "ar1",
                   scale.fix = TRUE,
                   data = respl)
```

:::

## Model covariance{.smaller}

```{r}
#| results: asis

tblun <- model3un %>%
  tbl_regression(exponentiate = TRUE)
tblex <- model3ex %>%
  tbl_regression(exponentiate = TRUE)
tblar1 <- model3ar1 %>%
  tbl_regression(exponentiate = TRUE)


# merge tables
tbl_merge_ex1 <-
  tbl_merge(
    tbls = list(tblun, tblex, tblar1),
    tab_spanner = c("**Unstructured**", "**Exhangeable**", "**AR1**")
  ) %>%
  modify_header(label = "**Variable**")
tbl_merge_ex1
```

```{r}
#| echo: TRUE

t(summary(model3un)$corr["Estimate"])
summary(model3ex)$corr["Estimate"]
summary(model3ar1)$corr["Estimate"]
QICun <- QIC(model3un)[1]; QICex <- QIC(model3ex)[1]; QICar1 <- QIC(model3ar1)[1]
round(c(QICun, QICex, QICar1),3)
```
## Working corr with OR parameterization{.smaller}

* Is modeling the association between two binary responses with a correlation coefficient inappropriate?
* We can model the **within-subject** association between the **two responses** in terms of a common log odds ratio, $\alpha$,
$$
\begin{aligned}
\text{logOR}(Y_{i1}, Y_{i2})&=\text{log}\left\{\frac{\Pr(Y_{i1}=1,Y_{i2}=1)\Pr(Y_{i1}=0,Y_{i2}=0)}{\Pr(Y_{i1}=1,Y_{i2}=0)\Pr(Y_{i1}=0,Y_{i2}=1)}\right\} \\
&= \alpha
\end{aligned}
$$

* Odds ratio parameterization for exchangeable working correlation
```{r}
#| echo: TRUE

modelOR <- ordgee(ordered(resp) ~ trt1 + visit + clinic2, 
                  id = id, 
                  mean.link = "logit", 
                  corstr = "exchangeable",
                  data = respl)
```


## Comparison

::: {.column width="50%"}
Correlation parameterization
```{r}
summary(model3ex)$coef[,1:2]
summary(model3ex)$corr
```

:::

::: {.column width="5%"}
:::

::: {.column width="40%"}
OR parameterization
```{r}
summary(modelOR)$mean[,1:2]
summary(modelOR)$cor[,1:2]
```

:::


#### Interpretation{.smaller}

::: {style="font-size: 75%;"}
* The log odds of good respiratory status for the treatment group compared to the placebo group is $0.79$ ($\text{OR (95% CI)}=2.21 (1.24, 3.92)$)
* The log odds of good respiratory status for patients in clinic 2 compared to clinic 1 is $0.92$ ($\text{OR (95% CI)}=2.51 (1.41, 4.47)$)

:::



## Ex: Clinical trial of antibiotics for leprosy{.smaller}

* The dataset consists of **count** data from a placebo-controlled clinical trial of 30 patients with leprosy at the Eversley Childs Sanitorium in the Philippines
* Participants in the study were randomized to either of two antibiotics (denoted treatment drug A and B) or to a placebo (denoted treatment drug C)
* Prior to receiving treatment, baseline data on the number of leprosy bacilli at six sites of the body where the bacilli tend to congregate were recorded for each patient
* After several months of treatment, the number of leprosy bacilli at six sites of the body were recorded a second time
* The outcome variable is the total **count** of the number of leprosy bacilli at the six sites
* In this study, the question of main scientific interest is whether treatment with antibiotics (drugs A and B) reduces the abundance of leprosy bacilli at the six sites of the body when compared to placebo (drug C)


## Ex: Clinical trial of antibiotics for leprosy{.smaller}
```{r}
#| echo: TRUE
lepr <- read.table(here("datasets", "leprosy.txt"), header = TRUE)
head(lepr, 20)
```


## Ex: Clinical trial of antibiotics for leprosy{.smaller}

Convert to long format
```{r}
#| echo: TRUE

leprl <- lepr %>%
  pivot_longer(cols = starts_with("y"),
               names_to = "time",
               names_prefix = "y",
               values_to = "count") %>%
  mutate(time = as.numeric(time) - 1, # time = 0 at baseline, time = 1 post-treatment
         # create dummy variables with drug C as ref category
         drugA = ifelse(drug == "Drug A", 1, 0), 
         drugB = ifelse(drug == "Drug B", 1, 0),
         # create drug x time interaction variables 
         timeA = time * drugA,
         timeB = time * drugB)
```

## Ex: Descriptive statistics{.smaller}
```{r}
#| echo: TRUE
leprl %>%
  group_by(drug, time) %>%
  summarise(meancount = mean(count), varcount = var(count))

```
* Descriptive statistics show that the variance is substantially greater than the mean at each time point for all three groups
* The Poisson assumption of $\text{Var}(Y_{ij}) = \mu_{ij}$ is not appropriate
* Instead, we consider
$$
\text{Var}(Y_{ij}) = \phi\mu_{ij}
$$
and estimate $\phi >1$ from the data (*Overdispersion factor*)


## Ex: Marginal model with log link{.smaller}
First, we will fit the following marginal model
$$
\small\text{log}E(Y_{ij})=\text{log}\mu_{ij} = \beta_{1} + \beta_{2}\text{Time}_{ij} + \beta_3\text{Time}_{ij} \times \text{DrugA}_{i} + \beta_4\text{Time}_{ij} \times \text{DrugB}_{i} 
$$
where 

* $Y_{ij}$ is the count of the number of leprosy bacilli for the $i$th patient in the $j$th occasion ($j = 1,2$)
* $\text{DrugA}_{i} = 1$ if patient was randomized to receive Drug A and $0$ otherwise
* $\text{DrugB}_{i} = 1$ if patient was randomized to receive Drug B and $0$ otherwise
* $\text{Time}_{ij} = 1$ for post-treatment measurement occasion and $0$ for baseline
* Because patients were **randomized**, we omit the main effects of treatment
* The working correlation matrix is $2 \times 2$ with
$$
\text{Corr}(Y_{i1}, Y_{i2}) = \alpha
$$

## Ex: Marginal model with log link{.smaller}
```{r}
#| echo: TRUE
modfit <- geeglm(count ~ time + timeA + timeB, data=leprl, 
                 id = id, 
                 family=poisson("log"), 
                 corstr="exch")
summary(modfit)
```

<br>

## Interpretation {.smaller}

* The expected count of leprosy bacilli at baseline in the placebo group is $e^{\beta_{1}} = e^{2.37} = 10.73$
* The expected count of leprosy bacilli at follow-up in the placebo group is $e^{\beta_{1}+\beta_2} = e^{2.37-0.003} = 10.70$
<!-- <div class = "green"> -->
<!-- `r text_spec("Question 2", color = "#095427", bold = T)` -->
<!-- <br> -->
<!-- What is the expected count of leprosy bacilli at **follow-up** in the placebo group? -->

<!-- a) 6.10 -->
<!-- b) 6.52 -->
<!-- c) 10.70 -->

* $e^{\beta_2}=0.997$ is the rate ratio of leprosy bacilli, comparing follow-up period to baseline in the placebo group
* $e^{\beta_2+\beta_{3}} = 0.568$ is the rate ratio of leprosy bacilli, comparing follow-up period to baseline in drug A
* $e^{\beta_2+\beta_{4}} = 0.608$ is the rate ratio of leprosy bacilli, comparing follow-up period to baseline in drug B



## Hypothesis testing {.smaller}
The coefficients $\beta_{3}$ and $\beta_{4}$ represent the difference between the changes in the log expected rates, comparing drug A and B to the placebo. 

Does treatment with antibiotics significantly reduce the abundance of leprosy bacilli?

$$
\text{H}_{0}: \beta_{3} = \beta_{4} = 0
$$
Let $\hat\beta = (\hat\beta_{3}, \hat\beta_{4})^{T}$. Then, the **multivariate Wald statistics** for the above hypothesis is
$$
W^{2} = \hat\beta\left\{\text{Cov}(\hat\beta)\right\}^{-1}\hat\beta
$$
which has a $\chi^{2}$ distribution with $2$ degrees of freedom.

## Hypothesis testing {.smaller}
Multivariate Wald statistic for $\text{H}_{0}: \beta_{3} = \beta_{4} = 0$
```{r}
#| echo: TRUE

ABest <- summary(modfit)$coeff[3:4,1]
ABest
ABvcov <- vcov(modfit)[3:4,3:4]
ABvcov
wald <- as.numeric(t(ABest) %*% solve(ABvcov) %*% (ABest))  
waldp <- 1-pchisq(wald, df=2)
c(wald, waldp)
```

The multivariate Wald statistic is $W^{2}=7.34$ with $2$ degrees of freedom ($p = 0.025$). This indicates that treatment with antibiotics is effective.


## Hypothesis testing {.smaller}

Are the two antibiotics equally effective?
$$
\text{H}_{0}: \beta_{3} = \beta_{4}
$$
The Wald statistic for the above hypothesis is
$$
W^{2} = \frac{(\beta_{3}-\beta_{4})^{2}}{\text{Var}(\beta_{3}-\beta_{4})}
$$
which has a $\chi^{2}$ distribution with $1$ degree of freedom.

## Hypothesis testing {.smaller}
Wald statistics for $\text{H}_{0}: \beta_{3} = \beta_{4}$
```{r}
#| echo: TRUE
waldnum <- ABest[1] - ABest[2]
waldden <- ABvcov[1,1] + ABvcov[2,2] - 2 * ABvcov[1,2]
waldstat <- waldnum^{2} / waldden
waldstatp <- 1-pchisq(waldstat, df=1)
c(waldstat, waldstatp)
```
Thus, we cannot reject the null hypothesis that the two antibiotics are equally effective in reducing the number of leprosy bacilli.

## Ex: a more parsimonious model {.smaller}
To obtain a common estimate of the log rate ratio, comparing both antibiotics (drugs A and B) to placebo, we can fit the reduced model

$$
\text{log}E(Y_{ij})=\text{log}\mu_{ij} = \beta_{1} + \beta_{2}\text{Time}_{ij} + \beta_3\text{Time}_{ij} \times \text{Trt}_{i}
$$

```{r}
#| echo: TRUE
leprl2 <- leprl %>%
  # create a new indicator variable for treatment (either drug A or drug B)
  mutate(trt = ifelse(drugA == 1 | drugB == 1, 1, 0),
         # interaction variable between time and treatment
         timetrt = time * trt)

modfit2 <- geeglm(count ~ time + timetrt, data=leprl2, 
                  id = id, 
                  family=poisson("log"), 
                  corstr="exch")
```

## Ex: a more parsimonious model {.smaller}
```{r}
#| echo: TRUE
summary(modfit2)
```


* The common estimate of the log rate ratio is $-0.528$
* The rate ratio is $e^{-0.528}=0.59$ with 95% confidence interval, $0.40$ to $0.87$, indicating that treatment with antibiotics significantly reduces the average number of leprocy bacilli when compared to placebo




## Marginal models for clustered data {.smaller}

* In some situations, responses may be correlated between units within the same cluster
* Examples are:
  + Members in the same family
  + Patients who seek care from the same doctor
  + Students in the same school
  + Teeth in patients
* Working correlation structures that make sense for these situations are the independent and exchangeable
* However, an often neglected assumption of GEE is that the size of the cluster needs to be independent of the outcomes
  + When this assumption is violated, results can be biased
  
## Example: Clinical attachment loss {.smaller}

* Clinical attachment loss (CAL) is a dental marker that measures the periodontal health of a tooth
* Higher CAL measurement indicates worse progress of periodontal disease
* Research question: Is metabolic syndrome (MetS) related to periodontitis (measured by CAL)?

```{r}
#| echo: FALSE

dd <- data.frame(
  Variable = c("subject", "tooth", "cal5", "age", "college", "smoking", "mets", "numteeth"),
  Type = rep("numeric", 8),
  Description = c("Unique subject identifier",
                  "Tooth identifier excluding third molars (wisdom teeth), 2-15, 18-31",
                  "Clinical attachment loss (CAL) on each tooth if > 5 milimeters",
                  "Age at time of exam",
                  "Indicator of college education",
                  "History of smoking, 1 = current or former smoker, 0 = never-smoker",
                  "Presence or absence of metabolic syndrome",
                  "Number of teeth at baseline")
)
kable(dd) %>%
  kable_minimal()
```
## Example: Clinical attachment loss {.smaller}

```{r}
#| echo: TRUE

cal <- read.csv(here("datasets", "cal.csv"))
head(cal)
length(unique(cal$subject))

ggplot(cal, aes(x = numteeth)) + 
  geom_bar() + 
  labs(y = "Count", x = "Number of teeth",
       main = "Distribution of number of teeth per patient")
```
## Informative cluster size (ICS)

* ICS occurs when cluster size (number of teeth) varies and is related to the outcome (CAL)

```{r}
cal %>%
  group_by(subject) %>%
  mutate(meanCAL = mean(cal5)) %>%
  ungroup() %>%
  group_by(numteeth, meanCAL) %>%
  tally(name = "frq") %>%
  ggplot(aes(x = numteeth, y = meanCAL, size = frq)) + 
  geom_point(alpha = 0.5, color="cornflowerblue") +
  scale_size(breaks = c(5, 10, 20, 30), range = c(1, 15), name="Frequency of patients") + 
  scale_x_continuous(breaks=c(0,5,10,15,20,25)) +
  theme_bw() + 
  theme(legend.position="bottom") + 
  labs(y="Proportion of CAL>5mm per patient", 
       x="Number of teeth") 
```

## Cluster weighted GEE{.smaller}

* To remove bias due to ICS, we estimate the parameters using a weighted estimating equations where the weights are inverse of cluster size, $1/n_i$

$$
\sum_{i=1}^{N}\frac{1}{n_i}\sum_{j=1}^{n_i}D^{T}_{ij}V_{ij}^{-1}(y_{ij}-\mu_{ij})=0.
$$
* $V_{ij}$ is assumed to be independent

```{r}
#| echo: TRUE
cwgee <- geeglm(cal5 ~ mets + college + age + smoking,
                id = subject,
                weight = 1/numteeth,
                family = binomial(link = "logit"),
                corstr = "ind",
                scale.fix = TRUE,
                data = cal)
```

## CWGEE vs GEE results
```{r}
geeex <- geeglm(cal5 ~ mets + college + age + smoking,
                id = subject,
                family = binomial(link = "logit"),
                corstr = "exchangeable",
                scale.fix = TRUE,
                data = cal)
geeind <- geeglm(cal5 ~ mets + college + age + smoking,
                id = subject,
                family = binomial(link = "logit"),
                corstr = "ind",
                scale.fix = TRUE,
                data = cal)

tblunwex <- geeex %>%
  tbl_regression(exponentiate = TRUE)
tblunwind <- geeind %>%
  tbl_regression(exponentiate = TRUE)
tblcw <- cwgee %>%
  tbl_regression(exponentiate = TRUE)


# merge tables
tbl_merge_ex2 <-
  tbl_merge(
    tbls = list(tblunwex, tblunwind, tblcw),
    tab_spanner = c("**GEE (Exch)**", "**GEE (Ind)**", "**CWGEE**")
  ) %>%
  modify_header(label = "**Variable**")
tbl_merge_ex2
```
## Conclusion on marginal models 

* More useful for **population averages**
* Addresses scientific questions that are concerned with changes in the (transformed) mean response over time in the study population
* Estimation via GEE (semi-parametric)
* Care with often overlooked assumptions
  + Informative cluster size
  + Informative visit times

