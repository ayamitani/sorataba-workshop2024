---
title: "Module 3: Mixed effects models"
author: "Aya Mitani"
format:
  revealjs:
    theme: theme.scss
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    embed-resources: true
---

```{r}
library(here)
library(tidyverse)
library(nlme)
library(lme4)
library(geepack)
library(kableExtra)
library(gtsummary)
library(texreg)
library(AICcmodavg)
knitr::opts_knit$set(root.dir = here())
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
theme_set(theme_bw(base_size = 20)) # Use black/white theme and increase font size for all ggplot 
```


# Linear mixed effects models

## Linear mixed effects models{.smaller}

* So far, we have discussed models for longitudinal data where changes in the mean response can be expressed as,

$$
E(Y_{i}|X_{i}) = X_{i}\beta,
$$
and where the primary goal is to make inferences about the population regression parameters, $\beta$

* Specification of this regression model was completed by making additional assumptions about the structure of $\text{Cov}(Y_{i})=\Sigma_{i}$

* We now consider an alternative, but closely related, approach for analyzing longitudinal data using **linear mixed effects models**

## Basic concept{.smaller}

* Some subset of the regression parameters **vary randomly** from one individual (cluster) to another, thereby accounting for sources of **natural heterogeneity** in the population
* Individuals in the population are assumed to have their own **subject-specific** mean response trajectories over time
* The mean response is modeled as a combination of population characteristics, $\beta$, that are assumed to be **shared** by all individuals, and subject-specific effects, $b_i$, that are **unique** to a particular individual

## Linear mixed effects model{.smaller}

Using vector and matrix notation, the **linear mixed effects model** is expressed as

$$
Y_{i} = X_{i}\beta + Z_{i}b_{i} + \epsilon_{i}
$$
The three components of the mixed effects models are:

::: {style="font-size: 75%;"}

<a> **Fixed effects** </a> 

* Variables that are constant across individuals
* $X_{i}$ is a $(n_{i} \times p)$ matrix of **covariates**
* $\beta$ is a $(p \times 1)$ vector of **fixed** effects

<a> **Random effects** </a> 

* Variables that vary among individuals
* $Z_{i}$ is a $(n_{i} \times q)$ matrix of **covariates**, with $q \le p$ (Usually, subset of $X_{i}$)
  + Usually, the columns of $Z_{i}$ are a subset of the columns of 
* $b_{i}$ is a $(q \times 1)$ vector of **random** effects

<a> **Errors** </a> 

* $\epsilon_{i}$ is a $(n_{i} \times 1)$ vector of **errors**

:::

## Linear mixed effects model{.smaller}
  
Additionally,

* The random effects, $b_{i}$, are assumed to be **independent** of the covariates, $X_{i}$, and has a multivariate normal distribution with mean 0 and covariance matrix $G$
  + $E(b_{i})=0$ and $\text{Cov}(b_{i})=G$ 
* The errors, $\epsilon_{i}$, are assumed to be **independent** of $b_{i}$, and has a multivariate normal distribution with mean 0 and covariance matrix $R_{i}$
  + $E(\epsilon_{i})=0$ and $\text{Cov}(\epsilon_{i})=R_{i}$
  + Further, we usually assume that $R_{i}$ is the diagonal matrix $\sigma^{2}I_{n_{i}}$
  + It is possible to assume $R_{i}$ to have a non-diagonal covariance structure, but only for **random intercepts models** 

## Linear mixed effects model{.smaller}

* With the inclusion of some random effects, the covariances among repeated measures can be expressed as **functions of time**
* Covariance pattern models do not distinguish the different sources of variability that have an impact on the covariance
* Linear mixed effects models 
  + explicitly distinguish **between-subject** and **within-subject** sources of variability
  + can also be used to obtain **predictions** of individual growth trajectories over time
  + are well suited for analyzing inherently **unbalanced** longitudinal data 
  
## Mean functions of LMM{.smaller}
The **conditional** or **subject-specific** mean of $Y_{i}$ is given by

$$
E(Y_{i}|b_{i}) = X_{i}\beta + Z_{i}b_{i},
$$
while the **marginal** or **population-averaged** mean of $Y_{i}$ is given by

$$
\begin{align}
E(Y_{i}) 
&= \mu_{i} \\
&= E\left\{E(Y_{i}|b_{i})\right\} \\
&= E(X_{i}\beta + Z_{i}b_{i}) \\
&= X_{i}\beta + Z_{i}E(b_{i}) \\
&= X_{i}\beta.
\end{align}
$$

## Covariance of LMM{.smaller}

In a similar way, we can distinguish between conditional and marginal covariances.

The **conditional covariance** of $Y_{i}$, given $b_{i}$, is 

$$
\text{Cov}(Y_{i}|b_{i}) = \text{Cov}(\epsilon_{i}) = R_{i},
$$

while the **marginal covariance** of $Y_{i}$, averaged over the distribution of $b_{i}$ is

$$
\begin{align}
\text{Cov}(Y_{i})
&= \text{Cov}(X_{i}\beta + Z_{i}b_{i} + \epsilon_{i}) \\
&= \text{Cov}(Z_{i}b_{i} + \epsilon_{i}) \\
&= \text{Cov}(Z_{i}b_{i}) + \text{Cov}(\epsilon_{i}) \\
&= Z_{i}\text{Cov}(b_{i})Z^{T}_{i} + \text{Cov}(\epsilon_{i}) \\
&= Z_{i}GZ^{T}_{i} + R_{i} \\
&= Z_{i}GZ^{T}_{i} + \sigma^{2}I_{n_{i}}.
\end{align}
$$

Note that this is **not** a diagonal matrix. $\text{Cov}(Y_{i})$ will have non-zero off-diagonal elements, thereby accounting for the **correlation among the repeated measurements** on the same individuals in a longitudinal study.

## Example: random intercepts model{.smaller}

We will revisit the dental study.

```{r}
dental <- read.table(here("datasets", "dentalstudy.txt"), header = TRUE)
dentallong <- dental %>%
  pivot_longer(cols = starts_with("distance"),
               names_to = "age",
               # this removes "distance_age" from the values
               names_prefix = "distance_age", 
               values_to = "distance",
  ) %>%
  # convert age to numeric
  mutate(age = as.numeric(age),
         agestar = age - 8) %>%
  # create new variable indicating measurement occasion
  group_by(id) %>%
  mutate(time = row_number()) %>%
  ungroup() 

ggplot(data = dentallong, aes(x = age, y = distance, color = gender)) +
  geom_point(size = 1.5) +    
  # add mean trajectory for each treatment group
  stat_summary(aes(group = gender), fun = mean, geom="line") + 
  labs(y = "Distance", x = "Age", color = "Gender", fill = "Gender") + 
  scale_fill_brewer(palette = "Dark2") + 
  scale_color_brewer(palette = "Dark2") 
```


## Example: random intercepts model{.smaller}

* The simplest LMM is one in which a single random intercepts is introduced
* $Z_{i}$ is now a $(n_{i} \times 1)$ vector of 1's
* $b_{i}$ is a subject-specific deviation around the population intercept, $\beta_0$, with $E(b_{i}) = 0$ and  $\text{Var}(b_{i}) = \sigma^{2}_{b}$
* The measurement error is assumed to be random with $E(\epsilon_{ij}) = 0$ and $\text{Var}(\epsilon_{ij}) = \sigma^{2}$

* We will fit the following LMM with random intercepts across subjects
$$
Y_{ij} = \beta_{0} + \beta_{1}\text{Age}^{*}_{ij} + \beta_{2}\text{Gender}_{i} + \beta_{3}\text{Age}^{*}_{ij} \times \text{Gender}_{i} + b_i + \epsilon_{ij}
$$
where $\text{Age}^{*} = \text{Age} - 8 \in (0,2,4,6)$.

## Example: LMM in R

There are two popular packages in R to fit LMM: 

 + `nlme` and `lmer`
```{r}
#| echo: TRUE

# random intercepts model with nlme package
library(nlme)
fit1 <- lme(distance ~ agestar * gender, random = ~ 1 | id,
            data = dentallong)

# same random intercepts model can be fit with lmer function in lme4 package
library(lme4)
fit1lmer <- lmer(distance ~ agestar * gender + (1|id), 
                 data = dentallong)
```


## Example: LMM in R{.smaller}
```{r}
#| echo: TRUE

summary(fit1)
```

* `lme()` (and `lmer()`) uses REML (Restricted Maximum Likelihood estimation) by default to fit LMM
* Likelihood based statistics can be used: AIC, BIC, likelihood ratio test


## Example: LMM in R{.smaller}
```{r}
#| echo: TRUE

summary(fit1)
```

* Marginal mean for Females: $E(Y_{ij}|X_{ij}) = 21.2 + 0.48\text{Age}^{*}_{ij}$
* Marginal mean for Males: $E(Y_{ij}|X_{ij}) = 22.6 + 0.78\text{Age}^{*}_{ij}$


## Example: Marginal covariance structure{.smaller}

* Note that 
  + $\hat{\sigma}_b = 1.82$ and $\hat{\sigma}_{\epsilon} = 1.39$
  
::: {style="font-size: 85%;"}
* The marginal variance of each response is given by
$$
\begin{align}
\text{Var}(Y_{ij})
&= \text{Var}(X^{T}_{ij}\beta + b_{i} + \epsilon_{ij}) \\
&= \text{Var}(b_{i} + \epsilon_{ij}) \\
&= \text{Var}(b_{i}) + \text{Var}(\epsilon_{ij}) \\
&= \sigma^{2}_{b} + \sigma^{2}_{\epsilon} = 5.22
\end{align}
$$
* The marginal covariance between any pair of responses, $Y_{ij}$ and $Y_{ik}$, is given by
$$
\begin{align}
\text{Cov}(Y_{ij},Y_{ik})
&= \text{Cov}(X^{T}_{ij}\beta + b_{i} + \epsilon_{ij}, X^{T}_{ik}\beta + b_{i} + \epsilon_{ik}) \\
&= \text{Cov}(b_{i} + \epsilon_{ij}, b_{i} + \epsilon_{ik}) \\
&= \text{Cov}(b_{i}, b_{i}) + \underbrace{\text{Cov}(b_{i}, \epsilon_{ik})}_{0} + \underbrace{\text{Cov}(\epsilon_{ij}, \epsilon_{ik})}_{0} + \underbrace{\text{Cov}(\epsilon_{ij}, b_{i})}_{0} \\
&= \text{Cov}(b_{i}, b_{i}) \\
&= \text{Var}(b_{i}) = \sigma^{2}_{b} = 3.30\\
\end{align}
$$

:::

## Example: Marginal covariance structure{.smaller}

The marginal **covariance matrix** of the repeated measurements has the following **compound symmetry** pattern: 

$$
\text{Cov}(Y_{i}) =
\left(\begin{array}{ccccc} 
\sigma^{2}_{b} + \sigma^{2} & \sigma^{2}_{b} & \sigma^{2}_{b} & ...& \sigma^{2}_{b}\\
\sigma^{2}_{b} & \sigma^{2}_{b} + \sigma^{2} & \sigma^{2}_{b} & ... & \sigma^{2}_{b}\\
\sigma^{2}_{b} & \sigma^{2}_{b} & \sigma^{2}_{b} + \sigma^{2} & ... & \sigma^{2}_{b}\\
\vdots & \vdots & \vdots  & \ddots  & \vdots \\
\sigma^{2}_{b} & \sigma^{2}_{b} & \sigma^{2}_{b} & ... & \sigma^{2}_{b} + \sigma^{2}\\
\end{array}\right).
$$
Note: This is the only covariance model that arises in both the patterned and random effects families.

```{r}
#| echo: TRUE

getVarCov(fit1, type = "marginal", individual = 1)
```
## Example: ICC {.smaller}

The correlation between $Y_{ij}$ and $Y_{ik}$ is
$$
\begin{align}
\text{Corr}(Y_{ij}, Y_{ik}) &= \frac{\text{Cov}(Y_{ij}, Y_{ik})}{\sqrt{\text{Var}(Y_{ij})\text{Var}(Y_{ik})}}\\
&= \frac{\sigma_{b}^{2}}{\sqrt{(\sigma_{b}^{2} + \sigma_{\epsilon}^{2})(\sigma_{b}^{2} + \sigma_{\epsilon}^{2})}}\\
&= \frac{\sigma_{b}^{2}}{\sigma_{b}^{2} + \sigma_{\epsilon}^{2}} = 0.63
\end{align}
$$
This correlation is referred to as **within-subject** correlation or **intraclass** correlation.

```{r}
#| echo: TRUE
getVarCov(fit1)
fit1$sigma^2
```

## Simulated visualization of 10 subjects with 5 time points 
```{r}
sim.rand.int <- function(n, sdint, sderr){
  int <- rep(rnorm(n, mean = 0, sd = sdint), each = 5)
  err <- rnorm(n * 5, mean = 0, sd = sderr)
  t <- rep(0:4, n)
  id <- rep(1:n, each = 5)
  y <- t + int + err
  out.data <- data.frame(scen = paste0(sdint, sderr), id = id, y = y, t = t)
  return(out.data)
}
scen11 <- sim.rand.int(n = 10, sdint = 1, sderr = 1)
scen10 <- sim.rand.int(n = 10, sdint = 2, sderr = 0)
scen01 <- sim.rand.int(n = 10, sdint = 0, sderr = 2)
scen21 <- sim.rand.int(n = 10, sdint = 2, sderr = 1)
allscens <- rbind(scen11, scen10, scen01, scen21)
allscens$scen <- factor(allscens$scen, levels = c("11", "20", "02", "21"),
                        ordered = TRUE, labels=c("sigma[b] == 0 ~~ sigma[epsilon] == 0", 
                                                 "sigma[b] == 2 ~~ sigma[epsilon] == 0",
                                                 "sigma[b] == 0 ~~ sigma[epsilon] == 2",
                                                 "sigma[b] == 2 ~~ sigma[epsilon] == 1"))
p <- ggplot(allscens, aes(y = y, x = t, group = id)) + 
  geom_line(aes(color = scen), linewidth = 1) + 
  labs(y = "y", x = "Time") + 
  facet_wrap(~ scen, labeller = label_parsed) + 
  theme(legend.position = "none") 
p
```


## Prediction{.smaller}

* In many applications where longitudinal data arise, we focus on inference of fixed effects, $\beta$
* However, in some longitudinal studies, we may want to **predict** subject-specific profiles
* Because the linear mixed effects model explicitly distinguishes between fixed and random effects, we can obtain predictions of **subject-specific effects**, $b_{i}$, or of the **subject-specific response trajectories**, $X_{i}\beta + Z_{i}b_{i}$

The best predictor of $b_{i}$ is known as the **best linear unbiased predictor (BLUP)**,
$$
E(b_{i}|Y_{i})=GZ^{T}_{i}\Sigma_{i}^{-1}(Y_{i}-X_{i}\hat\beta),
$$
where $\Sigma_{i} = \text{Cov}(Y_{i}) =  Z_{i}GZ^{T}_{i} + R_{i}$.  

When we replace the unknown covariance parameters by their REML estimates, we obtain the **empirical BLUP**,
$$
\hat{b}_{i}=\hat{G}Z_{i}^{T}\hat\Sigma_{i}^{-1}(Y_{i}-X_{i}\hat\beta).
$$
Given the empirical BLUP, $\hat{b}_{i}$, we can also obtain the $i$th subject's predicted response profile as:
$$
\hat{Y}_{i}=X_{i}\hat\beta+Z_{i}\hat{b}_{i}.
$$

## Prediction{.smaller}
Coefs of fixed effects
```{r}
#| echo: TRUE
coef(summary(fit1)) 
```


::: {.column width="49%"}
($\hat{\beta}_0 + \hat{b}_i$), and coefs of fixed effects
```{r}
#| echo: TRUE
coef(fit1)[1:15,] 
```
:::

::: {.column width="49%"}
Empirical BLUP
```{r}
#| echo: TRUE
ranef(fit1)[1:20,] 
```
:::


## Prediction{.smaller}
Here is a plot of the estimated **marginal** and **conditional** means by gender.
```{r}
condmean <- predict(fit1) # gives you the conditional means
margmean <- predictSE(fit1, dentallong)$fit # gives you the marginal means
preds <- as.data.frame(cbind(dentallong, condmean, margmean))
ggplot(data = preds, aes(x = agestar, y = condmean, group = id)) +
  geom_line(aes(color = gender), linetype = 3, alpha = 0.7) + 
  geom_line(aes(x = agestar, y = margmean, color = gender), size = 2) + 
  labs(y = "Distance", x = "Age", color = "Gender") + 
  scale_fill_brewer(palette = "Dark2") + 
  scale_color_brewer(palette = "Dark2") 
```


## Example: random intercepts and slopes model{.smaller}

* We can allow additional regression coefficients to vary randomly
* A very **flexible**, yet **parsimonious**, class of random effects covariance structures become available by allowing a subset of regression coefficients (other than just the intercept) to vary randomly

$$
Y_{ij} = \beta_{0} + \beta_{1}\text{Age}^{*}_{ij} + \beta_{2}\text{Gender}_{i} + \beta_{3}\text{Age}^{*}_{ij} \times \text{Gender}_{i} + b_{i0} + b_{i1}\text{Age}^{*}_{ij} + \epsilon_{ij}
$$

The vector of random effects for individual $i$, $b_{i}$, is $2 \times 1$:
$$
b_{i} = 
\left(\begin{array}{cc} 
b_{0i} \\
b_{1i} 
\end{array}\right) \quad \text{with} \quad
E(b_{i}) = 
\left(\begin{array}{cc} 
0 \\
0
\end{array}\right) \\\
$$

$$
\text{and} \quad \begin{align}
G = \text{Cov}(b_{i}) =
\left(\begin{array}{cc} 
\text{Var}(b_{0i}) & \text{Cov}(b_{1i},b_{1i}) \\
\text{Cov}(b_{0i},b_{1i}) & \text{Var}(b_{1i}) 
\end{array}\right) 
=\left(\begin{array}{cc} 
g_{00} & g_{10} \\
g_{10} & g_{11}
\end{array}\right) \\
\end{align}
$$

## Example: Covariance{.smaller}

So, remember that $\text{Cov}(Y_{i}) = Z_{i}GZ^{T}_{i} + \sigma^{2}I_{n_{i}}$?

We can further show that 

$$
\text{Var}(Y_{ij}) = g_{11} + 2t_{ij}g_{12} + t_{ij}^{2}g_{22} + \sigma^{2}
$$
and

$$
\text{Cov}(Y_{ij},Y_{ik}) = g_{11} + (t_{ij}+t_{ik})g_{12} + t_{ij}t_{ik}g_{22}.
$$
Therefore, $\text{Cov}(Y_{i})$ can be expressed as a **function of time**, $t_{ij}$!

* The variance, $\text{Var}(Y_{ij})$ can increase or decrease as a **quadratic function** of the times of measurement
* The magnitude of the covariance between a pair of responses $\text{Cov}(Y_{ij},Y_{ik})$ depends on the **time separation** between $t_{ij}$ and $t_{ik}$

Thus, we can construct a very **flexible** covariance structure in a more **parsimonious** way with a linear mixed effects model than with a covariance pattern model. 


## Example: LMM in R{.smaller}
```{r}
#| echo: TRUE
## random intercepts + random slopes

fit2 <- lme(distance ~ agestar * gender, random = ~ agestar | id, 
            data = dentallong)
summary(fit2)
```

## Example: LMM in R{.smaller}


```{r}
#| echo: TRUE
getVarCov(fit2)
getVarCov(fit2, type = "conditional", individual = 1)
getVarCov(fit2, type = "marginal", individual = 1)
```


## Prediction{.smaller}
Coefs of fixed effects
```{r}
#| echo: TRUE
coef(summary(fit2)) 
```


::: {.column width="49%"}
($\hat{\beta} + \hat{b}_i$), and coefs of fixed effects
```{r}
#| echo: TRUE
coef(fit2)[1:15,] 
```
:::

::: {.column width="49%"}
Empirical BLUP
```{r}
#| echo: TRUE
ranef(fit2)[1:15,] 
```
:::


## Prediction{.smaller}
Here is a plot of the estimated **marginal** and **conditional** means by gender.
```{r}
condmean <- predict(fit2) # gives you the conditional means
margmean <- predictSE(fit2, dentallong)$fit # gives you the marginal means
preds <- as.data.frame(cbind(dentallong, condmean, margmean))
ggplot(data = preds, aes(x = agestar, y = condmean, group = id)) +
  geom_line(aes(color = gender), linetype = 3, alpha = 0.7) + 
  geom_line(aes(x = agestar, y = margmean, color = gender), size = 2) + 
  labs(y = "Distance", x = "Age", color = "Gender") + 
  scale_fill_brewer(palette = "Dark2") + 
  scale_color_brewer(palette = "Dark2") 
```


## Random intercepts and covariance pattern model{.smaller}

* We can introduce more flexibility by explicitly by modeling the correlation structure in $R_{i}$, i.e. move beyond $ R_{i} = \sigma^{2}I_{n_{i}}$ and permit the $\epsilon_{i}$ to be correlated
* The hybrid random intercepts and covariance pattern model looks like:
$$
Y_{ij} = X^{T}_{ij}\beta + b_{0i} + U_{i}(t_{ij}) + \epsilon^{*}_{ij},
$$
where
  - $b_{0i}$ is an individual-specific random intercept with $E(b_{0i})=0$ and $\text{Var}(b_{0i}) = \sigma^{2}_{b}$
  - $U_{i}(t_{ij})$ is a covariance pattern (serial dependence) term 
  - We assume $E(\epsilon^{*}_{ij})=0$ and $\text{Var}(\epsilon^{*}_{ij}) = \sigma^{2}$
  - Finally, we assume $U_{i}(t_{ij})$ are assumed to have a normal distribution, with zero mean, variance $\sigma^{2}_{u}$, and correlation: $\text{Corr}\left\{U_{i}(t_{ij}),U_{i}(t_{ik})\right\} = \rho(|t_{ij}-t_{ik}|)$

* $\epsilon^{*}_{ij}$ captures the variation in error terms beyond that is explained by $U_{i}(t_{ij})$


## Example: random intercepts + auto-regressive errors {.smaller}

```{r}
#| echo: TRUE

fit3 <- lme(distance ~ agestar * gender, random = ~ 1 | id, 
            corr = corCAR1(form = ~ agestar | id),
            data = dentallong)
getVarCov(fit3)
getVarCov(fit3, type = "conditional", individual = 1)
getVarCov(fit3, type = "marginal", individual = 1)
```




## Example: random intercepts + exponential errors {.smaller}

```{r}
#| echo: TRUE

fit4 <- lme(distance ~ agestar * gender, random = ~ 1 | id, 
            corr = corExp(form = ~ agestar | id),
            data = dentallong)
getVarCov(fit4)
getVarCov(fit4, type = "conditional", individual = 1)
getVarCov(fit4, type = "marginal", individual = 1)
```


## Example: random intercepts + heteroskedasticity errors across age {.smaller}

```{r}
#| echo: TRUE

fit5 <- lme(distance ~ agestar * gender, random = ~ 1 | id, 
            weights = varIdent(form = ~ 1 | agestar),
            data = dentallong)
getVarCov(fit5)
getVarCov(fit5, type = "conditional", individual = 1)
getVarCov(fit5, type = "marginal", individual = 1)
```



## Example: random intercepts + heteroskedasticity errors across gender {.smaller}

```{r}
#| echo: TRUE

fit6 <- lme(distance ~ agestar * gender, random = ~ 1 | id, 
            weights = varIdent(form = ~ 1 | gender),
            data = dentallong, method = "ML")
getVarCov(fit6)
getVarCov(fit6, type = "conditional", individual = 1) # female
getVarCov(fit6, type = "conditional", individual = 27) # male
```

## Example: random intercepts + heteroskedasticity errors across gender {.smaller}

```{r}
#| echo: TRUE

getVarCov(fit6, type = "marginal", individual = 1) # female
getVarCov(fit6, type = "marginal", individual = 27) # male
```



## Comparing AIC and BIC {.smaller}
```{r}
#| echo: TRUE
#| eval: FALSE
AIC(fit) # gives you AIC
BIC(fit) # gives you BIC
```

```{r}
#| echo: FALSE
getAICBIC <- function(fit){
  AIC <- AIC(fit)
  BIC <- BIC(fit)
  return(c(AIC, BIC))
}

allfits <- list(fit1, fit2, fit3, fit4, fit5, fit6)
AICBIClist <- list()
for(i in 1:6){
  AICBIClist[[i]] <- getAICBIC(allfits[[i]])
}

AICBICmat <- do.call("rbind", AICBIClist)
depmodel <- c(
  "1. Random intercepts + ind errors",
  "2. Random intercepts and slopes + ind errors",
  "3. Random intercepts + AR1 errors",
  "4. Random intercepts + exp errors",
  "5. Random intercepts + heteroske ind errors (age)",
  "6. Random intercepts + heteroske ind errors (gender)"
)
AICBICmat <- as.data.frame(cbind(depmodel, AICBICmat))
names(AICBICmat) <- c("Covariance model", "AIC", "BIC")

AICBICmat %>%
  mutate(AIC = as.numeric(AIC),
         BIC = as.numeric(BIC)) %>%
  kbl(digits = 2) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

* Model 6 (Random intercepts + heterogeneous variance terms across gender) has the lowest AIC and BIC



## Comparing coefficient estimates and standard errors for $\beta_{3}$ {.smaller}
```{r}
getcoef <- function(sumfit){
  intcoef <- coef(sumfit)[4,1:2]
  return(intcoef)
}
allsums <- list(summary(fit1), summary(fit2), summary(fit3), summary(fit4), summary(fit5), summary(fit6))
coeflist <- list()
for(i in 1:6){
  coeflist[[i]] <- getcoef(allsums[[i]])
}
coefmat <- do.call("rbind", coeflist)
coefmat <- as.data.frame(cbind(depmodel, coefmat))
names(coefmat) <- c("Covariance model", "Estimate", "StdError")

coefmat %>%
  mutate(Estimate = as.numeric(Estimate),
         StdError = as.numeric(StdError)) %>%
  kbl(digits = 3) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

* $\hat\beta_{3}$ are very similar across all models
* Model 6 has the smallest standard errors


# Generalized linear mixed effects models

## Generalized linear mixed effects models{.smaller}

* GLMs can be extended to longitudinal (clustered) data by allowing a subset of the regression coefficients to **vary randomly** from one individual (cluster) to another
* The concept of GLMMs are similar to LMM
  + However, introduction of random effects in GLMs is more complicated

GLMM formulation

$$
g\left\{E(Y_{ij}|b_{i})\right\} = X_{ij}^{T}\beta + Z_{ij}^{T}b_{i}
$$


## Interpretation of regression parameters{.smaller}

* The introduction of random effects has important implications for the interpretation of the regression coefficients, $\beta$, in GLMM
* In GLMM, $\beta$ has **subject-specific** interpretations
* $\beta$ represents the **within-subject** changes in covariates on changes in an individual's **transformed** mean response, while holding the remaining covariates constant

For example, consider a simple logistic regression model with randomly varying intercepts:
$$
\text{log}\left\{\frac{\Pr(Y_{ij}=1|b_{i})}{\Pr(Y_{ij}=0|b_{i})}\right\} = \beta_{1} + \beta_{2}X_{ij} + b_{i},
$$
where $b_{i} \sim N(0, g_{11})$.

## Interpretation of regression parameters{.smaller}

When $X_{ij} = x$, then the log odds of a positive response $Y_{ij}=1$ is
$$
\text{log}\left\{\frac{\Pr(Y_{ij}=1|b_{i}, X_{ij}=x)}{\Pr(Y_{ij}=0|b_{i}, X_{ij}=x)}\right\} = \beta_{1} + \beta_{2}x + b_{i}.
$$
Similarly, when $X_{ij}=x+1$, then the log odds of $Y_{ij}=1$ is
$$
\text{log}\left\{\frac{\Pr(Y_{ij}=1|b_{i}, X_{ij}=x+1)}{\Pr(Y_{ij}=0|b_{i}, X_{ij}=x+1)}\right\} = \beta_{1} + \beta_{2}(x+1) + b_{i}.
$$
For any individual $i$, the **difference in log odds** of a positive response for a unit increase in $X_{ij}$ is
$$
\begin{aligned}
\text{log}\left\{\frac{\Pr(Y_{ij}=1|b_{i}, X_{ij}=x+1)}{\Pr(Y_{ij}=0|b_{i}, X_{ij}=x+1)}\right\} &- \text{log}\left\{\frac{\Pr(Y_{ij}=1|b_{i}, X_{ij}=x)}{\Pr(Y_{ij}=0|b_{i}, X_{ij}=x)}\right\} \\
&= \beta_{1} + \beta_{2}(x+1) + b_{i} - \left\{\beta_{1} + \beta_{2}x + b_{i}\right\} \\
&= \beta_{2}.
\end{aligned}
$$

## Interpretation of regression parameters{.smaller}
Furthermore,
$$
\begin{aligned}
\text{log}\left\{\frac{\Pr(Y_{ij}=1|b_{i}, X_{ij}=x+1)}{\Pr(Y_{ij}=0|b_{i}, X_{ij}=x+1)} \bigg/ \frac{\Pr(Y_{ij}=1|b_{i}, X_{ij}=x)}{\Pr(Y_{ij}=0|b_{i}, X_{ij}=x)}\right\}
&= \beta_{2} \\
\left\{\frac{\Pr(Y_{ij}=1|b_{i}, X_{ij}=x+1)}{\Pr(Y_{ij}=0|b_{i}, X_{ij}=x+1)} \bigg/ \frac{\Pr(Y_{ij}=1|b_{i}, X_{ij}=x)}{\Pr(Y_{ij}=0|b_{i}, X_{ij}=x)}\right\}
&= e^{\beta_{2}}.
\end{aligned}
$$
Therefore, $e^{\beta_{2}}$ is the odds ratio of a positive response for a unit increase in $X_{ij}$ for any given **individual** having some unobservable underlying propensity to respond positively, $b_{i}$.



## Subject-specific covariatess{.smaller}

* Because the components of the fixed effects, $\beta$, have interpretations that depend on holding $b_{i}$ fixed, they are often referred to as **subject-specific** regression coefficients
* The subject-specific interpretations of $\beta$ is more natural for covariates that **vary within an individual** (within-subject or time-varying covariates).
  + For example, age, BMI, smoking status, etc.
  + But covariates are assumed to be independent of the random effects (**assumption of endogeneity**) $\rightarrow$ more on this next week
  + Interpretation of $\beta$ is the change in an **individual's log odds of response** for a unit increase in $X_{ij}$
  


## Between-subject covariatess{.smaller}

* What if $X_{ij}$ is a between-subject covariate, such as gender, treatment, or exposure group?
* Then, the change in the value of the covariate requires also a change in index $i$ of $X_{ij}$ to $X_{i'j}$ for $i \neq i'$
  + However, $\beta$ then becomes confounded with $b_{i} - b_{i'}$
  + So we need to assume $b_{i} = b_{i'}$
  + The interpretation of $\beta$ is the change in log odds of response for two different individuals who **happen to have the same value** for the random effect, but who differ by one unit in the covariate $X_{ij}$



## Special case of the identity link functions{.smaller}

* GLMM with the identity link function is equivalent to LMM
* With the identity link function, the regression coefficients in the model for the conditional means,
$$
E(Y_{ij}|X_{ij}, b_{i}) = X_{ij}^{T}\beta + Z_{ij}^{T}b_{i},
$$
also happen to have interpretation in terms of the population means, since
$$
\begin{aligned}
E(Y_{ij}|X_{ij}) &= E\left\{E(Y_{ij}|X_{ij}, b_{i})\right\} \\
&= E(X_{ij}^{T}\beta + Z_{ij}^{T}b_{i}) \\
&= X_{ij}^{T}\beta + Z_{ij}^{T}E(b_{i}) \\
&= X_{ij}^{T}\beta
\end{aligned}
$$
when averaged over all individuals in the study population.

## GLMM with non-identity link functions{.smaller}

However, for the non-identity link function, such as logit($\cdot$) or log($\cdot$), that are usually adopted for discrete responses, this relationship no longer holds. That is, if
$$
g\left\{E(Y_{ij}|X_{ij}, b_{i})\right\} =  X_{ij}^{T}\beta + Z_{ij}^{T}b_{i}
$$
where $g(\cdot)$ is a non-identity link function, then
$$
g\left\{E(Y_{ij}|X_{ij})\right\} \neq  X_{ij}^{T}\beta. 
$$

For non-identity link functions, the regression coefficients in GLMM and **marginal models** have quite distinct interpretations, and they have different targets of inference.


## Example: Respiratory study {.smaller}
Percent of good respiratory status over time by clinic
```{r}
resp <- read.table(here("datasets", "respir.txt"), header = TRUE)
respl <- resp %>%
  pivot_longer(cols = starts_with("y"),
               names_to = "visit",
               names_prefix = "y",
               values_to = "resp") %>%
  mutate(visit = as.numeric(visit),
         trt1 = ifelse(trt == "A", 1, 0),
         clinic2 = ifelse(clinic == 2, 1, 0))
respl %>%
  group_by(visit, trt, clinic) %>%
  mutate(pcty = mean(resp)) %>%
  ggplot(aes(y = pcty, x = visit, color = as.factor(trt))) +
  geom_point(size = 2.5) + 
  geom_line(size = 1.5) + 
  facet_grid(~clinic) + 
  ylim(0,1) + 
  labs(y = "Percent of good respiratory status", x = "Visit", color = "Treatment")
```

## Example: Random intercepts model {.smaller}

* In R, the `glmer()` function in `lme4` package is most popular for GLMM

```{r}
#| echo: TRUE

glmm1 <- glmer(resp ~ trt1 + visit + clinic2 + (1|id),
                data = respl,
                nAGQ = 50, # number of quadrature points (default is 1)
                family = binomial("logit"))
```

* In maximum likelihood estimation, GLMM has **no closed-form solutions**
* Instead, we use a **numerical integration technique** called **Gaussian quadrature** to approximate the marginal likelihood as a weighted sum with the number of **quadrature points**, $K$

* When $K = 1$, this technique is referred to as **Laplace** approximation
* The higher the $K$, the more accurate the approximation will be
* But the computational burden also increases with increasing $K$
  + `nAGQ > 1` option in `glmer()` is only available for random intercepts model

## Example: Random intercepts model {.smaller}
```{r}
#| echo: TRUE
summary(glmm1)
```



## Intra-cluster correlation{.smaller}
For binary GLMM, we can assume that $Y_{ij}$ is generated by an **unobserved latent continuous variable**, $L_{ij}$, such that
$$
L_{ij} = X_{ij}\beta + b_{j} + \epsilon_{ij}
$$
where $b_{j}\sim N(0,\sigma^{2}_{b})$, and $\epsilon_{ij}\sim Logistic(0,1)$ with $\text{var}(\epsilon_{ij})=\pi^{2}/3$.

* Without loss of generosity, we can assume that
$$
Y_{ij}=
\begin{cases}
1 \text{ if } L_{ij}>0 \\
0 \text{ if } L_{ij}\leq 0
\end{cases} \quad
$$
* We can then quantify the relative magnitudes of the between-cluster and within-cluster sources of variability using the intra-cluster correlation
$$
\rho = \text{Corr}(L_{ij}, L_{i'j}) = \frac{\sigma^{2}_{b}}{\sigma^{b}_{2}+\pi^{2}/3}  = \frac{5.039}{5.039+\pi^{2}/3} = 0.605
$$
* Note that $\rho$ is the marginal correlation among the latent variables $L_{ij}$ and $L_{i'j}$ and **not** the marginal correlation among the observed variables $Y_{ij}$ and $Y_{i'j}$ 

## Comparing marginal vs conditional model

* In module 2, we fit a marginal model with working exchangeable assumption
* ORs between marginal and conditional models are quite different

```{r}
model3ex <- geeglm(resp ~ trt1 + visit + clinic2,
                 id = id,
                 family = binomial(link = "logit"),
                 corstr = "exchangeable",
                 scale.fix = TRUE,
                 data = respl)
tblex <- model3ex %>%
  tbl_regression(exponentiate = TRUE)
tblglmm1 <- glmm1 %>%
  tbl_regression(exponentiate = TRUE)
# merge tables
tbl_merge_ex1 <-
  tbl_merge(
    tbls = list(tblex, tblglmm1),
    tab_spanner = c("**Marginal model**", "**Conditional model**")
  ) %>%
  modify_header(label = "**Variable**")
tbl_merge_ex1
```

## Example: Random intercepts and slopes model{.smaller}

* Need to use the default **Laplace** approximation 

```{r}
#| echo: TRUE
glmm2 <- glmer(resp ~ trt1 * visit + clinic2 + (1 + visit|id),
                data = respl,
                family = binomial("logit"))
summary(glmm2)
```

## Example: Nested random effects{.smaller}

* Instead of including the clinic effect as a fixed effect, we can include it as a random effect 
* We will fit the following **three-level** GLMM
$$
\begin{aligned}
\text{logit}\left\{E(Y_{ijk}|b^{(3)}_{k},b^{(2)}_{ik})\right\}&=\beta_1 + \beta_2 \text{Trt}_i + \beta_3 \text{Visit}_{ij} +b^{(2)}_{ik} + b^{(3)}_{k}
\end{aligned}
$$
where $Y_{ijk}=1$ if patient $i$ in clinic $k$ at time $j$ has the outcome and $Y_{ijk} =0$ otherwise and $b_{ik}^{(2)} \sim N(0, \sigma_{2}^2)$, $b_{k}^{(3)} \sim N(0,\sigma_{3}^2)$.

```{r}
#| echo: TRUE
glmm3 <- glmer(resp ~ trt1 * visit + (1|id) + (1|clinic2),
                data = respl,
                family = binomial("logit"))
```

## Example: Nested random effects{.smaller}
```{r}
#| echo: TRUE

summary(glmm3)
```

## Example: Intra-class correlation {.smaller}
What is the correlation between patient $i$ and patient $i'$ in the same clinic $k$?

Note that
$$
\begin{aligned}
\text{Cov}(L_{ijk}, L_{i'jk}) 
&= \text{Cov}(b_{ik}^{(2)}+b_{k}^{(3)}+\epsilon_{ijk}, b_{ik}^{(2)}+b_{k}^{(3)}+\epsilon_{i'jk}) \\
&= \text{Cov}(b_{ik}^{(2)},b_{ik}^{(2)}) + \text{Cov}(b_{k}^{(3)},b_{k}^{(3)}) \\
&= \text{Var}(b_{ik}^{(2)}) + \text{Var}(b_{k}^{(3)}) \\
&= \sigma_{2}^{2} +  \sigma_{3}^{2},
\end{aligned}
$$

and 
$$
\begin{aligned}
\text{Var}(L_{ijk})&=\text{Var}(b_{ik}^{(2)}+b_{k}^{(3)}+\epsilon_{ijk})\\
&=\text{Var}(b_{ik}^{(2)})+\text{Var}(b_{k}^{(3)})+\pi^2/3\\
&= \sigma_{2}^{2} +  \sigma_{3}^{2}+\pi^2/3,
\end{aligned}
$$

## Example: Intra-class correlation {.smaller}

which means that
$$
\begin{aligned}
\text{Corr}(Y_{ijk}, Y_{i'jk}) 
&= \frac{\hat\sigma_{2}^{2} +  \hat\sigma_{3}^{2}}{\sqrt{(\hat\sigma_{2}^{2} +  \hat\sigma_{3}^{2}+\pi^2/3)(\hat\sigma_{2}^{2} + \hat\sigma_{3}^{2}+\pi^2/3)}}.
\end{aligned}
$$
Therefore,
```{r}
sum.glmm3 <- summary(glmm3)
sum.glmm3$varcor
var2 <- as.numeric(sum.glmm3$varcor$id)
var3 <- as.numeric(sum.glmm3$varcor$clinic2)
intracl <- (var2 + var3) / (var2 + var3 + pi^2/3)
intracl
```


